{{- if and .Values.modelServers.llmRejection.enabled (not .Values.modelServers.llmRejection.remoteModelServer.enabled) }}
# Autogenerated by a script. DO NOT EDIT.
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llm-rejection
  annotations:
    {{- include "ri-detection-resources.annotations" . | nindent 4 }}
spec:
  predictor:
    {{- with .Values.images.imagePullSecrets }}
    imagePullSecrets:
      {{- toYaml . | nindent 8 }}
    {{- end }}
    minReplicas: {{ .Values.modelServers.llmRejection.minReplicas }}
    maxReplicas: {{ .Values.modelServers.llmRejection.maxReplicas }}
    securityContext:
      runAsUser: 1000
      runAsGroup: 1000
      fsGroup: 1000
    containers:
      - name: llm-rejection
      {{- if .Values.modelServers.llmRejection.useCachedArtifact.enabled }}
        image: "{{ .Values.modelServers.llmRejection.useCachedArtifact.dockerImageOverride.registry }}/{{ .Values.modelServers.llmRejection.useCachedArtifact.dockerImageOverride.name }}"
        imagePullPolicy: {{ .Values.modelServers.llmRejection.useCachedArtifact.dockerImageOverride.pullPolicy }}
      {{- else }}
        image: "{{ .Values.images.modelServerImage.registry}}/{{ .Values.images.modelServerImage.name }}"
        imagePullPolicy: {{ .Values.images.modelServerImage.pullPolicy }}
      {{- end }}
        env:
        {{- if not .Values.modelServers.llmRejection.useCachedArtifact.enabled }}
          - name: HUGGINGFACE_API_KEY
            valueFrom:
              secretKeyRef:
                name: {{ .Values.secrets.existingIntegrationSecretsName }}
                key: huggingfaceAPIKey
        {{- end }}
        ports:
          - name: rest
            protocol: TCP
            containerPort: 8080
        command:
          - "run_server"
        args:
          - "--model-config-path=/model_server/model-settings.json"
          - "--cache-dir=/model_cache"
        startupProbe:
          httpGet:
            path: /v2/models/llm-rejection/ready
            port: 8080
          failureThreshold: 20
          periodSeconds: 5
        readinessProbe:
          httpGet:
            path: /v2/models/llm-rejection/ready
            port: 8080
          periodSeconds: 5
        resources:
          requests:
            memory: {{ .Values.modelServers.llmRejection.resources.requests.memory }}
            cpu: {{ .Values.modelServers.llmRejection.resources.requests.cpu }}
          limits:
            memory: {{ .Values.modelServers.llmRejection.resources.limits.memory }}
            cpu: {{ .Values.modelServers.llmRejection.resources.limits.cpu }}
        volumeMounts:
          - name: llm-rejection-config
            mountPath: "/model_server"
            readOnly: true
          - name: llm-rejection-model-cache
            mountPath: "/model_cache"
            readOnly: false
    volumes:
      # Volumes are defined at the Pod level, then mounted into containers within that Pod
      - name: llm-rejection-config
        configMap:
          name: {{ include "ri-detection-resources.fullname" . }}-llm-rejection-conf
          items:
            - key: "model-settings.json"
              path: "model-settings.json"
      - name: llm-rejection-model-cache
        emptyDir: { }
{{- end }}
